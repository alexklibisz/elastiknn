{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions about effects of multiple shards, nodes on query speed\n",
    "\n",
    "Trying to demonstrate two issues in this notebook:\n",
    "\n",
    "1. Searches on an index with $n$ shards and search threads are not $n$ times faster than searches on an index with 1 shard.\n",
    "2. Searches on a single node with $n$ shards and search threads are substantially slower than searches on 2 nodes each with $n/2$ shards.\n",
    "\n",
    "## Background\n",
    "\n",
    "My assumption was always that adding shards would roughly linearly decrease the query latency. I.e., 8 shards should be ~8x faster than 1 shard. However, when I measured it for a few cases, I noticed that 8 shards aren't even ~2x faster than 1 shard, hence issue #1. Then I ran into issue #2 in the process of debugging issue #1.\n",
    "\n",
    "I have a hunch about #1. Specifically, there's some overhead in serializing/transferring the query from the client to the server and some more overhead in collecting the results from each shard and serializing/transferring them back to the client. This overhead is inherentally serial, so it negates some of the speedup from having more shards. This is reflected if you look at the sampled search thread state in VisualVM. The threads spend about half their runtime in a \"Parked\" state. AFAIK, this means they just don't have any work to do. You can keep them busy by executing multiple search requests in parallel. I posted some screenshots in this Github issue: https://github.com/alexklibisz/elastiknn/issues/220\n",
    "\n",
    "I have no idea why #2 is happening.\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "To run Elasticsearch, I'm using a docker-compose stack with:\n",
    "- one master node which stores no data with a 1GB heap\n",
    "- 1 or 2 data nodes, each with 8 search threads and a 4GB heap\n",
    "\n",
    "To generate an index and queries, I'm using the [Amazon Reviews Dataset](https://jmcauley.ucsd.edu/data/amazon/), specifically the 5-core Clothing and Jewelry reviews, containing 278k reviews. I index the full review text in a `text` field, use the shorter review summary to build a `term` query, and run the queries one at a time.\n",
    "\n",
    "I'm running this on my Dell XPS 9570 laptop, ubuntu 20.04, with 32GB memory, an SSD, and this CPU: i7-8750H CPU @ 2.20GHz × 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘reviews_Clothing_Shoes_and_Jewelry_5.json.gz’ already there; not retrieving.\n",
      "\n",
      "46M\treviews_Clothing_Shoes_and_Jewelry_5.json.gz\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset.\n",
    "!wget -nc http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Clothing_Shoes_and_Jewelry_5.json.gz\n",
    "!du -hs *.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for benchmarking.\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "import json\n",
    "import gzip \n",
    "\n",
    "es = Elasticsearch(hosts=[\"localhost:9200\"])\n",
    "review_file = 'reviews_Clothing_Shoes_and_Jewelry_5.json.gz'\n",
    "\n",
    "def build_index(shards):\n",
    "  index = f\"ix-{shards}\"\n",
    "  body = {\n",
    "    \"settings\": {\n",
    "      \"number_of_shards\": shards,\n",
    "      \"number_of_replicas\": 0\n",
    "    }\n",
    "  }\n",
    "  mapping = {\n",
    "    \"properties\": {\n",
    "      \"review\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  if es.indices.exists(index):\n",
    "      print(f\"Index {index} already exists\")\n",
    "      return index\n",
    "  es.indices.create(index, body=body)\n",
    "  es.indices.put_mapping(mapping, index)\n",
    "  \n",
    "  def bulkgen():\n",
    "    with gzip.open(review_file, 'r') as g:\n",
    "      for l in tqdm(g, desc=f\"Indexing with {shards} shards\"):\n",
    "        d = eval(l)\n",
    "        yield {\"_op_type\": \"index\", \"_index\": index, \"review\": d['reviewText']}\n",
    "      \n",
    "  (_, errors) = bulk(es, bulkgen(), chunk_size=5000, max_retries=9)\n",
    "  assert len(errors) == 0, errors\n",
    "  es.indices.refresh(index=index)\n",
    "  es.indices.forcemerge(index=index, max_num_segments=1)\n",
    "  return index\n",
    "\n",
    "def search(index, n=1000, size=10):\n",
    "  # Pre-generate queries so that it's not the bottleneck.\n",
    "  queries = []\n",
    "  \n",
    "  def querygen():\n",
    "    with gzip.open(review_file, 'r') as g:\n",
    "      for l in g:\n",
    "        d = eval(l)\n",
    "        yield {\n",
    "          \"query\": {\n",
    "            \"match\": {\n",
    "              \"review\": {\n",
    "                \"query\": d['summary'],\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "          \n",
    "  queries = list(islice(querygen(), n))\n",
    "          \n",
    "  for query in tqdm(queries, desc=f\"Running {n} queries on index {index}\"):\n",
    "    res = es.search(index=index, body=query, size=size, _source=False)\n",
    "    \n",
    "def start_cluster(data_nodes=1):\n",
    "  !docker-compose up -d --force-recreate --scale elasticsearch_data=$data_nodes\n",
    "  !sleep 30\n",
    "  !echo '\\n'\n",
    "  !curl localhost:9200/_cat/nodes?v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Single data node, single shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating parallel-shards-question_elasticsearch_data_1 ... \n",
      "Recreating elasticsearch_master                          ... \n",
      "\u001b[1Beating elasticsearch_master                          ... \u001b[32mdone\u001b[0m\u001b[1A\u001b[2K\\n\n",
      "ip         heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name\n",
      "172.18.0.3            7          27   8    1.45    0.76     0.31 dilrt     -      07d88c0d6541\n",
      "172.18.0.2           61          27   6    1.45    0.76     0.31 mr        *      elasticsearch_master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing with 1 shards: 278677it [00:27, 10262.50it/s]\n",
      "Running 50000 queries on index ix-1: 100%|██████████| 50000/50000 [03:13<00:00, 258.51it/s]\n"
     ]
    }
   ],
   "source": [
    "start_cluster(1)\n",
    "index = build_index(1)\n",
    "search(index, n=50000, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single data node, eight shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing with 8 shards: 278677it [00:19, 13962.05it/s]\n",
      "Running 50000 queries on index ix-8: 100%|██████████| 50000/50000 [03:02<00:00, 274.16it/s]\n"
     ]
    }
   ],
   "source": [
    "index = build_index(8)\n",
    "search(index, n=50000, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two data nodes, eight shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating parallel-shards-question_elasticsearch_data_1 ... \n",
      "Recreating elasticsearch_master                          ... \n",
      "\u001b[2BCreating parallel-shards-question_elasticsearch_data_2   ... mdone\u001b[0m\u001b[2A\u001b[2K\n",
      "\u001b[1Bting parallel-shards-question_elasticsearch_data_2   ... \u001b[32mdone\u001b[0m\\n\n",
      "ip         heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name\n",
      "172.18.0.4            7          43  12    3.53    2.44     1.23 dilrt     -      7bd2b11572db\n",
      "172.18.0.3            7          43  12    3.53    2.44     1.23 dilrt     -      74cca019a317\n",
      "172.18.0.2           21          43  16    3.53    2.44     1.23 mr        *      elasticsearch_master\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing with 8 shards: 278677it [00:22, 12482.99it/s]\n",
      "Running 50000 queries on index ix-8: 100%|██████████| 50000/50000 [02:36<00:00, 318.85it/s]\n"
     ]
    }
   ],
   "source": [
    "start_cluster(2)\n",
    "index = build_index(8)\n",
    "search(index, n=50000, size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
