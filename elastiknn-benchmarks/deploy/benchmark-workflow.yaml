# Use envsubst to "render" the file.
# Seems like the most I can allocate on a c5.4xlarge is 15 CPUs and 26Gi memory.
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: elastiknn-benchmark-
spec:
  entrypoint: main
  arguments:
    parameters:
      - name: driverImage
      - name: elastiknnImage
      - name: awsAccessKeyId
      - name: awsSecretAccessKey
      - name: bucket
        value: elastiknn-benchmarks
      - name: searchBackend
        value: "lucene"
  templates:
    - name: main
      parallelism: 40 # EKS seems to choke somewhere around 50 nodes.
      steps:
        - - name: generate-experiments
            template: generate-experiments
        - - name: enqueue-experiments
            template: enqueue-experiments
        - - name: apply-configmap
            template: apply-configmap
        - - name: execute-lucene
            when: "{{workflow.parameters.searchBackend}} == lucene"
            template: execute-lucene
            arguments:
              parameters:
                - name: experiment
                  value: "{{item}}"
            withParam: "{{steps.enqueue-experiments.outputs.parameters.experiments}}"
        - - name: execute-elasticsearch
            when: "{{workflow.parameters.searchBackend}} == elasticsearch"
            template: execute-elasticsearch
            arguments:
              parameters:
                - name: experiment
                  value: "{{item}}"
            withParam: "{{steps.enqueue-experiments.outputs.parameters.experiments}}"
#        - - name: aggregate-results
#            template: aggregate-results

    # Scala app generates experiments, writes them to s3, generates a file containing their keys, writes the file to s3.
    - name: generate-experiments
      container:
        image: "{{workflow.parameters.driverImage}}"
        imagePullPolicy: "Always"
        args:
          - com.klibisz.elastiknn.benchmarks.Generate
          - --experimentsPrefix
          - experiments
          - --bucket
          - "{{workflow.parameters.bucket}}"
          - --keysKey
          - experiments/latest.json
        env:
          - name: AWS_ACCESS_KEY_ID
            value: "{{workflow.parameters.awsAccessKeyId}}"
          - name: AWS_SECRET_ACCESS_KEY
            value: "{{workflow.parameters.awsSecretAccessKey}}"

    # Cli app copies experiments to disk so argo can generate a pod for each of the experiments.
    - name: enqueue-experiments
      container:
        image: amazon/aws-cli
        args: ["s3", "cp", "--quiet", "s3://elastiknn-benchmarks/experiments/latest.json", "/tmp/latest.json"]
        env:
          - name: AWS_ACCESS_KEY_ID
            value: "{{workflow.parameters.awsAccessKeyId}}"
          - name: AWS_SECRET_ACCESS_KEY
            value: "{{workflow.parameters.awsSecretAccessKey}}"
      outputs:
        parameters:
          - name: experiments
            valueFrom:
              path: /tmp/latest.json

    # Create a configmap mounted by the Elasticsearch sidecar containers.
    - name: apply-configmap
      resource:
        action: apply
        manifest: |
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: esconfig
          data:
            elasticsearch.yml: |
              cluster.name: "no-cluster"
              network.host: 0.0.0.0

    # Execute experiments using Lucene backend.
    - name: execute-lucene
      inputs:
        parameters:
          - name: experiment
      retryStrategy:
        limit: 1
        retryPolicy: "Always"
      nodeSelector:
        beta.kubernetes.io/instance-type: c5.large
      container:
        image: "{{workflow.parameters.driverImage}}"
        imagePullPolicy: "Always"
        args:
          - com.klibisz.elastiknn.benchmarks.Execute
          - --experimentKey
          - "{{inputs.parameters.experiment}}"
          - --datasetsPrefix
          - data/processed
          - --resultsPrefix
          - results/raw
          - --bucket
          - "{{workflow.parameters.bucket}}"
        env:
          - name: AWS_ACCESS_KEY_ID
            value: "{{workflow.parameters.awsAccessKeyId}}"
          - name: AWS_SECRET_ACCESS_KEY
            value: "{{workflow.parameters.awsSecretAccessKey}}"
          - name: JAVA_OPTS
            value: >-
              -XX:MinRAMPercentage=5
              -XX:MaxRAMPercentage=80
              -XX:+HeapDumpOnOutOfMemoryError
              -Dcom.sun.management.jmxremote.ssl=false
              -Dcom.sun.management.jmxremote.authenticate=false
              -Dcom.sun.management.jmxremote.local.only=false
              -Dcom.sun.management.jmxremote.port=8097
              -Dcom.sun.management.jmxremote.rmi.port=8097
              -Djava.rmi.server.hostname=localhost
        ports:
          - name: jmx
            containerPort: 8097
            protocol: TCP
        resources:
          requests:
            cpu: 1
            memory: 2Gi
          limits:
            cpu: 1
            memory: 2Gi

    - name: execute-elasticsearch
      inputs:
        parameters:
          - name: experiment
      retryStrategy:
        limit: 1
        retryPolicy: "Always"
      steps:
        - - name: create-pvc
            template: create-pvc
        - - name: execute-elasticsearch-driver
            template: execute-elasticsearch-driver
            continueOn:
              failed: true
            arguments:
              parameters:
                - name: experiment
                  value: "{{inputs.parameters.experiment}}"
                - name: pvc-name
                  value: "{{steps.create-pvc.outputs.parameters.pvc-name}}"
        - - name: delete-pvc
            template: delete-pvc
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{steps.create-pvc.outputs.parameters.pvc-name}}"
        - - name: exit
            template: exit
            arguments:
              parameters:
                - name: code
                  value: "{{steps.execute-elasticsearch-driver.exitCode}}"

    - name: aggregate-results
      container:
        image: "{{workflow.parameters.driverImage}}"
        imagePullPolicy: "Always"
        env:
          - name: AWS_ACCESS_KEY_ID
            value: "{{workflow.parameters.awsAccessKeyId}}"
          - name: AWS_SECRET_ACCESS_KEY
            value: "{{workflow.parameters.awsSecretAccessKey}}"
        args:
          - com.klibisz.elastiknn.benchmarks.Aggregate
          - --resultsBucket
          - elastiknn-benchmarks
          - --resultsPrefix
          - results/raw
          - --aggregateBucket
          - elastiknn-benchmarks
          - --aggregateKey
          - results/aggregate/aggregate.csv

    - name: create-pvc
      resource:
        action: create
        manifest: |
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            generateName: "{{workflow.name}}-"
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 50Gi
            storageClassName: storage-10-iops
      outputs:
        parameters:
          - name: pvc-name
            valueFrom:
              jsonPath: '{.metadata.name}'

    # Have to delete and then patch in order to remove the finalizer that prevents deletion.
    # Otherwise this gets stuck in the `Terminating` state.
    - name: delete-pvc
      inputs:
        parameters:
          - name: pvc-name
      container:
        image: "argoproj/argoexec:v2.8.0"
        command: ["/bin/sh", "-c"]
        args:
          - |
            /bin/sh <<'EOSCRIPT'
            set -e
            kubectl delete pvc {{inputs.parameters.pvc-name}}
            kubectl patch --type merge -p  '{"metadata":{"finalizers":null}}' pvc {{inputs.parameters.pvc-name}}
            EOSCRIPT

    # Propagates an exit code. Used as last step so WF can continue on failed steps, do some cleanup, and _then_ fail.
    - name: exit
      inputs:
        parameters:
          - name: code
      container:
        image: alpine
        command: [sh, -c]
        args: ["exit {{inputs.parameters.code}}"]

    - name: execute-elasticsearch-driver
      inputs:
        parameters:
          - name: experiment
          - name: pvc-name
      volumes:
        - name: esdata
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"
        - name: esconfig
          configMap:
            name: esconfig
      securityContext:
        fsGroup: 1000 # Sets the correct permissions for mounted volume.
      container:
        image: "{{workflow.parameters.driverImage}}"
        imagePullPolicy: "Always"
        args:
          - com.klibisz.elastiknn.benchmarks.Execute
          - --experimentKey
          - "{{inputs.parameters.experiment}}"
          - --datasetsPrefix
          - "data/processed"
          - --resultsPrefix
          - "results/raw"
          - --bucket
          - "{{workflow.parameters.bucket}}"
          - --esUrl
          - http://localhost:9200
          - --shards
          - "4"
          - --parallelQueries
          - "1"
        env:
          - name: AWS_ACCESS_KEY_ID
            value: "{{workflow.parameters.awsAccessKeyId}}"
          - name: AWS_SECRET_ACCESS_KEY
            value: "{{workflow.parameters.awsSecretAccessKey}}"
          - name: JAVA_OPTS
            value: >-
              -XX:MinRAMPercentage=10
              -XX:MaxRAMPercentage=90
        resources:
          requests:
            cpu: 1
            memory: 2G
          limits:
            cpu: 1
            memory: 2G
      sidecars:
        - name: elastiknn
          image: "{{workflow.parameters.elastiknnImage}}"
          imagePullPolicy: "Always"
          env:
            - name: discovery.type
              value: single-node
            - name: ES_JAVA_OPTS
              value: >-
                -Xms15G
                -Xmx15G
          volumeMounts:
            - name: esdata
              mountPath: /usr/share/elasticsearch/data
              readOnly: false
            - name: esconfig
              mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
              subPath: elasticsearch.yml
              readOnly: true
          resources:
            requests:
              cpu: 14
              memory: 24G
            limits:
              cpu: 14
              memory: 24G
