# Use envsubst to "render" the file.
# Seems like the most I can allocate on a c5.4xlarge is 15 CPUs and 26Gi memory.
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: datasets-
spec:
  entrypoint: main
  onExit: exit-handler
  templates:
    - name: main
      parallelism: 1
      steps:
        - - name: create-pvc
            template: create-pvc
        - - name: create-dataset
            template: create-dataset
        - - name: sync-dataset
            template: sync-dataset
        - - name:
        - - name: execute-experiment
            template: execute-experiment
            arguments:
              parameters:
                - name: experiment
                  value: "{{item}}"
            withParam: "{{steps.generate-experiments.outputs.parameters.experiments}}"

    - name: generate-experiments
      container:
        image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/elastiknn-benchmarks-cluster.driver
        imagePullPolicy: "Always"
        args:
          - com.klibisz.elastiknn.benchmarks.Enqueue
          - --datasetsFilter
          - AnnbSift
          - --toFile
          - /tmp/exps.json
      outputs:
        parameters:
          - name: experiments
            valueFrom:
              path: /tmp/exps.json

    - name: apply-configmap
      resource:
        action: apply
        manifest: |
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: esconfig
          data:
            elasticsearch.yml: |
              cluster.name: "docker-cluster"
              network.host: 0.0.0.0

    - name: execute-experiment
      inputs:
        parameters:
          - name: experiment
      steps:
        - - name: create-pvc
            template: create-pvc
        - - name: execute-driver
            template: execute-driver
            continueOn:
              failed: true
            arguments:
              parameters:
                - name: experiment
                  value: "{{inputs.parameters.experiment}}"
                - name: pvc-name
                  value: "{{steps.create-pvc.outputs.parameters.pvc-name}}"
        - - name: delete-pvc
            template: delete-pvc
            arguments:
              parameters:
                - name: pvc-name
                  value: "{{steps.create-pvc.outputs.parameters.pvc-name}}"

    - name: create-pvc
      resource:
        action: create
        manifest: |
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            generateName: elastiknn-pvc-
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 21Gi
            storageClassName: storage-10-iops
      outputs:
        parameters:
          - name: pvc-name
            valueFrom:
              jsonPath: '{.metadata.name}'

    # Have to delete and then patch in order to remove the finalizer that prevents deletion.
    # Otherwise this gets stuck in the `Terminating` state.
    - name: delete-pvc
      inputs:
        parameters:
          - name: pvc-name
      container:
        image: "argoproj/argoexec:v2.8.0"
        command: ["/bin/sh", "-c"]
        args:
          - |
            /bin/sh <<'EOSCRIPT'
            set -e
            kubectl delete pvc {{inputs.parameters.pvc-name}}
            kubectl patch --type merge -p  '{"metadata":{"finalizers":null}}' pvc {{inputs.parameters.pvc-name}}
            EOSCRIPT

    - name: execute-driver
      inputs:
        parameters:
          - name: experiment
          - name: pvc-name
      nodeSelector:
        beta.kubernetes.io/instance-type: c5.4xlarge
      volumes:
        - name: esdata
          persistentVolumeClaim:
            claimName: "{{inputs.parameters.pvc-name}}"
        - name: esconfig
          configMap:
            name: esconfig
      securityContext:
        fsGroup: 1000 # Sets the correct permissions for mounted volume.
      container:
        image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/elastiknn-benchmarks-cluster.driver
        imagePullPolicy: "Always"
        args:
          - com.klibisz.elastiknn.benchmarks.Execute
          - --experimentJsonBase64
          - "{{inputs.parameters.experiment}}"
          - --datasetsBucket
          - elastiknn-benchmarks
          - --datasetsPrefix
          - "data/processed"
          - --resultsBucket
          - "elastiknn-benchmarks-cluster.results"
        env:
          # Technically bad practice, but easier than setting up KIAM.
          - name: AWS_ACCESS_KEY_ID
            value: ${AWS_ACCESS_KEY_ID}
          - name: AWS_SECRET_ACCESS_KEY
            value: ${AWS_SECRET_ACCESS_KEY}
        resources:
          requests:
            cpu: 1
            memory: 4G
          limits:
            cpu: 1
            memory: 4G
      sidecars:
        - name: elastiknn
          image: ${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/elastiknn-benchmarks-cluster.elastiknn
          imagePullPolicy: "Always"
          env:
            - name: discovery.type
              value: single-node
          volumeMounts:
            - name: esdata
              mountPath: /usr/share/elasticsearch/data
              readOnly: false
            - name: esconfig
              mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
              subPath: elasticsearch.yml
              readOnly: true
          resources:
            requests:
              cpu: 14
              memory: 22G
            limits:
              cpu: 14
              memory: 22G
